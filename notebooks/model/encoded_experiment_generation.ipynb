{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T08:40:22.680470Z",
     "start_time": "2024-11-04T08:40:22.677182Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from chinese_checkers.experience import Experience, ExperienceMetadata, ExperienceCatalog\n",
    "from chinese_checkers.reinforcement.dql import DQLAgent, DqlModelValidation\n",
    "from chinese_checkers.simulation import SimulationCatalog, SimulationMetadata, SimulationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da5912b-1730-4319-a19c-9de28e74f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_distributions(win_rewards: List[float], loss_rewards: List[float], title: str) -> None:\n",
    "    \"\"\"\n",
    "    Plots overlayed reward distributions for winning and losing games, displaying mean and standard deviation.\n",
    "\n",
    "    Args:\n",
    "        win_rewards (List[float]): Rewards from winning games.\n",
    "        loss_rewards (List[float]): Rewards from losing games.\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    win_mean = np.mean(win_rewards)\n",
    "    win_std = np.std(win_rewards)\n",
    "    loss_mean = np.mean(loss_rewards)\n",
    "    loss_std = np.std(loss_rewards)\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Winning Player - Mean Reward: {win_mean:.2f}, Standard Deviation: {win_std:.2f}\")\n",
    "    print(f\"Losing Player - Mean Reward: {loss_mean:.2f}, Standard Deviation: {loss_std:.2f}\")\n",
    "\n",
    "    # Plot overlayed normalized reward distributions with legend\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(win_rewards, bins=60, alpha=0.6, color='blue', edgecolor='black', density=True,\n",
    "             label=f\"Winning Game: Rewards\\nMean = {win_mean:.2f}, Std = {win_std:.2f}\")\n",
    "    plt.hist(loss_rewards, bins=60, alpha=0.6, color='green', edgecolor='black', density=True,\n",
    "             label=f\"Losing Game: Rewards\\nMean = {loss_mean:.2f}, Std = {loss_std:.2f}\")\n",
    "\n",
    "    plt.title(f\"Overlayed {title} Distributions for Winning and Losing Games\")\n",
    "    plt.xlabel(\"Reward\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee7434c8-8af0-4f74-a94a-7ebb2807f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5292b10bb2a4fc17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T08:40:23.262716Z",
     "start_time": "2024-11-04T08:40:23.254592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SimulationMetadata(player_count=2, board_size=4, max_game_length=1000, winning_player='0', name='bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=1000, winning_player='3', name='bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=1000, winning_player='0', name='bad-player-3-bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=1000, winning_player='3', name='bad-player-3-bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=1000, winning_player='None', name='bad-player-3-bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=150, winning_player='0', name='bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=150, winning_player='3', name='bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=200, winning_player='3', name='bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=200, winning_player='0', name='bootstrap-simulation', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=200, winning_player='0', name='bootstrap-simulation-short', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=200, winning_player='3', name='bootstrap-simulation-short', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=300, winning_player='3', name='bootstrap-simulation-random-noise', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=300, winning_player='0', name='bootstrap-simulation-random-noise', version='v0.0.1'),\n",
       " SimulationMetadata(player_count=2, board_size=4, max_game_length=500, winning_player='3', name='bootstrap-vs-dql-v003', version='v0.0.1')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_catalog = SimulationCatalog()\n",
    "sim_metadata: List[SimulationMetadata] = sim_catalog.list_datasets()\n",
    "sim_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d89559c0773e00aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T08:40:27.074256Z",
     "start_time": "2024-11-04T08:40:24.630085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10772 datasets for winning_players: ['0', '3'] and player_count: 2.\n"
     ]
    }
   ],
   "source": [
    "winning_players = [\"0\", \"3\"]\n",
    "player_count = 2\n",
    "\n",
    "simulations: List[SimulationData] = [\n",
    "    dataset\n",
    "    for metadata in sim_metadata\n",
    "    if metadata.winning_player in winning_players\n",
    "        and metadata.player_count == player_count\n",
    "    for dataset in sim_catalog.load_dataset(metadata)\n",
    "]\n",
    "print(f\"Found {len(simulations)} datasets for winning_players: {winning_players} and player_count: {player_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4311412-d8f1-4730-ad91-c58c6511b048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='0', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.1', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='0', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='0', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='3', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.1', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='3', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.1', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='3', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='3', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='0', name='bad-player-3-bootstrap-simulation', version='v0.0.1', generator_name='v0.0.1', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='0', name='bad-player-3-bootstrap-simulation', version='v0.0.1', generator_name='v0.0.1', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='0', name='bad-player-3-bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='0', name='bad-player-3-bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='3', name='bad-player-3-bootstrap-simulation', version='v0.0.1', generator_name='v0.0.1', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='1000', winning_player='3', name='bad-player-3-bootstrap-simulation', version='v0.0.1', generator_name='v0.0.1', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='0', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='0', name='bootstrap-simulation', version='v0.0.1', generator_name='v0.0.2', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='0', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.1', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='0', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.1', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='0', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='0', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.2', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='3', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.1', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='3', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.1', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='3', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='200', winning_player='3', name='bootstrap-simulation-short', version='v0.0.1', generator_name='v0.0.2', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='300', winning_player='3', name='bootstrap-simulation-random-noise', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='300', winning_player='3', name='bootstrap-simulation-random-noise', version='v0.0.1', generator_name='v0.0.2', current_player='3'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='500', winning_player='3', name='bootstrap-vs-dql-v003', version='v0.0.1', generator_name='v0.0.2', current_player='0'),\n",
       " ExperienceMetadata(player_count='2', board_size='4', max_game_length='500', winning_player='3', name='bootstrap-vs-dql-v003', version='v0.0.1', generator_name='v0.0.2', current_player='3')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_catalog = ExperienceCatalog()\n",
    "exp_catalog.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d05f155-1f2b-4fad-9f35-208fd6b0f767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation = simulations[10]\n",
    "# experiences = Experience.generate_experiences_from_simulation(\n",
    "#     simulation,\n",
    "#     generator_name=\"v0.0.2\"\n",
    "# )\n",
    "# games = simulation._to_game_sequence()\n",
    "# games[0].print(show_coordinates=True)\n",
    "# print(simulation.data.historical_moves[0])\n",
    "# print(experiences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "801d4a9415685cce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T08:40:27.786695Z",
     "start_time": "2024-11-04T08:40:27.655211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating experiences (Last generated: 251 experiences):   4%|‚ñç         | 468/10772 [1:33:16<34:13:40, 11.96s/it]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 0] Unable to synchronously open file (unable to lock file, errno = 0, error message = 'No error', Win32 GetLastError() = 33)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 8\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m simulation \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[0;32m      4\u001B[0m     experiences \u001B[38;5;241m=\u001B[39m Experience\u001B[38;5;241m.\u001B[39mgenerate_experiences_from_simulation(\n\u001B[0;32m      5\u001B[0m         simulation,\n\u001B[0;32m      6\u001B[0m         generator_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv0.0.3\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      7\u001B[0m     )\n\u001B[1;32m----> 8\u001B[0m     \u001B[43mexp_catalog\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_record_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexperiences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerating experiences (Last generated: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(experiences)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m experiences)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\Documents\\Stanford\\CS223\\ChineseCheckersGameEngine\\ChineseCheckersGameEngine\\src\\chinese_checkers\\catalog\\LocalH5Catalog.py:63\u001B[0m, in \u001B[0;36mLocalH5Catalog.add_record_list\u001B[1;34m(self, data_metadata_list, batch_size)\u001B[0m\n\u001B[0;32m     61\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing batch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;250m \u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m\u001B[38;5;250m \u001B[39mbatch_size\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_batches\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data_metadata \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[1;32m---> 63\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_record_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_metadata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Stanford\\CS223\\ChineseCheckersGameEngine\\ChineseCheckersGameEngine\\src\\chinese_checkers\\catalog\\LocalH5Catalog.py:78\u001B[0m, in \u001B[0;36mLocalH5Catalog._add_record_internal\u001B[1;34m(self, data_metadata)\u001B[0m\n\u001B[0;32m     75\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreated new dataset for metadata \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_metadata\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;66;03m# Append the record to the file\u001B[39;00m\n\u001B[1;32m---> 78\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mh5py\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43ma\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m h5file:\n\u001B[0;32m     79\u001B[0m     group \u001B[38;5;241m=\u001B[39m h5file\u001B[38;5;241m.\u001B[39mcreate_group(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(h5file\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     80\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m storable_data\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32m~\\Documents\\Stanford\\CS223\\ChineseCheckersGameEngine\\ChineseCheckersGameEngine\\venv\\Lib\\site-packages\\h5py\\_hl\\files.py:561\u001B[0m, in \u001B[0;36mFile.__init__\u001B[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001B[0m\n\u001B[0;32m    552\u001B[0m     fapl \u001B[38;5;241m=\u001B[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001B[0;32m    553\u001B[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001B[0;32m    554\u001B[0m                      alignment_threshold\u001B[38;5;241m=\u001B[39malignment_threshold,\n\u001B[0;32m    555\u001B[0m                      alignment_interval\u001B[38;5;241m=\u001B[39malignment_interval,\n\u001B[0;32m    556\u001B[0m                      meta_block_size\u001B[38;5;241m=\u001B[39mmeta_block_size,\n\u001B[0;32m    557\u001B[0m                      \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    558\u001B[0m     fcpl \u001B[38;5;241m=\u001B[39m make_fcpl(track_order\u001B[38;5;241m=\u001B[39mtrack_order, fs_strategy\u001B[38;5;241m=\u001B[39mfs_strategy,\n\u001B[0;32m    559\u001B[0m                      fs_persist\u001B[38;5;241m=\u001B[39mfs_persist, fs_threshold\u001B[38;5;241m=\u001B[39mfs_threshold,\n\u001B[0;32m    560\u001B[0m                      fs_page_size\u001B[38;5;241m=\u001B[39mfs_page_size)\n\u001B[1;32m--> 561\u001B[0m     fid \u001B[38;5;241m=\u001B[39m \u001B[43mmake_fid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muserblock_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfapl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfcpl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mswmr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mswmr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    563\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(libver, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    564\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_libver \u001B[38;5;241m=\u001B[39m libver\n",
      "File \u001B[1;32m~\\Documents\\Stanford\\CS223\\ChineseCheckersGameEngine\\ChineseCheckersGameEngine\\venv\\Lib\\site-packages\\h5py\\_hl\\files.py:247\u001B[0m, in \u001B[0;36mmake_fid\u001B[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001B[0m\n\u001B[0;32m    242\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    243\u001B[0m     \u001B[38;5;66;03m# Open in append mode (read/write).\u001B[39;00m\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001B[39;00m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;66;03m# existing one (ACC_EXCL)\u001B[39;00m\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 247\u001B[0m         fid \u001B[38;5;241m=\u001B[39m \u001B[43mh5f\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh5f\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mACC_RDWR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfapl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfapl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    248\u001B[0m     \u001B[38;5;66;03m# Not all drivers raise FileNotFoundError (commented those that do not)\u001B[39;00m\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m fapl\u001B[38;5;241m.\u001B[39mget_driver() \u001B[38;5;129;01min\u001B[39;00m (\n\u001B[0;32m    250\u001B[0m         h5fd\u001B[38;5;241m.\u001B[39mSEC2,\n\u001B[0;32m    251\u001B[0m         h5fd\u001B[38;5;241m.\u001B[39mDIRECT \u001B[38;5;28;01mif\u001B[39;00m direct_vfd \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    259\u001B[0m         h5fd\u001B[38;5;241m.\u001B[39mROS3D \u001B[38;5;28;01mif\u001B[39;00m ros3 \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m    260\u001B[0m     ) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "File \u001B[1;32mh5py\\\\_objects.pyx:54\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\\\_objects.pyx:55\u001B[0m, in \u001B[0;36mh5py._objects.with_phil.wrapper\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mh5py\\\\h5f.pyx:102\u001B[0m, in \u001B[0;36mh5py.h5f.open\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: [Errno 0] Unable to synchronously open file (unable to lock file, errno = 0, error message = 'No error', Win32 GetLastError() = 33)"
     ]
    }
   ],
   "source": [
    "random.shuffle(simulations)\n",
    "with tqdm(simulations, desc=\"Generating experiences\") as pbar:\n",
    "    for simulation in pbar:\n",
    "        experiences = Experience.generate_experiences_from_simulation(\n",
    "            simulation,\n",
    "            generator_name=\"v0.0.3\"\n",
    "        )\n",
    "        exp_catalog.add_record_list(experiences)\n",
    "        pbar.set_description(f\"Generating experiences (Last generated: {len(experiences)} experiences)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ffb134ed8f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_metadata: List[ExperienceMetadata] = exp_catalog.list_datasets()\n",
    "for metadata in exp_metadata:\n",
    "    if metadata.generator_name == \"v0.0.3\":\n",
    "        print(metadata)\n",
    "        # exp_catalog.delete_dataset(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1035db6a7a5df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences: List[Experience] = [\n",
    "    dataset\n",
    "    for metadata in exp_metadata\n",
    "    if metadata.winning_player in winning_players\n",
    "        and metadata.player_count == str(player_count)\n",
    "        and metadata.generator_name == \"v0.0.3\"\n",
    "        and metadata.current_player == \"0\"\n",
    "    for dataset in exp_catalog.load_dataset(metadata)\n",
    "]\n",
    "print(f\"Found {len(experiences)} datasets for winning_players: {winning_players} and player_count: {player_count}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed455c6d-7c85-43d9-8307-55caf5c04240",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda669efa9c39df",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_exps = [e for e in experiences if e.metadata.winning_player == \"0\"]\n",
    "loss_exps = [e for e in experiences if e.metadata.winning_player == \"3\"]\n",
    "\n",
    "\n",
    "plot_reward_distributions(\n",
    "    [e.data.reward.item() for e in win_exps], \n",
    "    [e.data.reward.item() for e in loss_exps], \n",
    "    \"Reward\"\n",
    ")\n",
    "\n",
    "# Plotting action distributions by dimension\n",
    "action_dims = experiences[0].data.action.shape[-1]\n",
    "p0_actions = [[] for _ in range(action_dims)]\n",
    "p3_actions = [[] for _ in range(action_dims)]\n",
    "\n",
    "for e in experiences:\n",
    "    if e.metadata.winning_player == \"0\":\n",
    "        for i in range(action_dims):\n",
    "            p0_actions[i].append(e.data.action[i].item())\n",
    "    elif e.metadata.winning_player == \"3\":\n",
    "        for i in range(action_dims):\n",
    "            p3_actions[i].append(e.data.action[i].item())\n",
    "\n",
    "# Plot normalized action distributions for each dimension\n",
    "for i in range(action_dims):\n",
    "    plot_reward_distributions(p0_actions[i], p3_actions[i], f\"Action Dimension {i+1}\")\n",
    "\n",
    "# Plotting state distributions by dimension\n",
    "state_dims = experiences[0].data.state.shape[-1]\n",
    "p0_states = [[] for _ in range(state_dims)]\n",
    "p3_states = [[] for _ in range(state_dims)]\n",
    "\n",
    "for e in experiences:\n",
    "    if e.metadata.winning_player == \"0\":\n",
    "        for i in range(state_dims):\n",
    "            p0_states[i].append(e.data.state[i].item())\n",
    "    elif e.metadata.winning_player == \"3\":\n",
    "        for i in range(state_dims):\n",
    "            p3_states[i].append(e.data.state[i].item())\n",
    "\n",
    "# Plot normalized state distributions for each dimension\n",
    "for i in range(state_dims):\n",
    "    plot_reward_distributions(p0_states[i], p3_states[i], f\"State Dimension {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6afdb-13c6-4b41-adba-948ddb70524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the number of dimensions in the action tensor\n",
    "action_dims = experiences[0].data.action.shape[-1]\n",
    "\n",
    "# Extract rewards for each dimension and player group\n",
    "p0_actions = [[] for _ in range(action_dims)]\n",
    "p3_actions = [[] for _ in range(action_dims)]\n",
    "\n",
    "# Populate action lists for each dimension\n",
    "for e in experiences:\n",
    "    if e.metadata.winning_player == \"0\":\n",
    "        for i in range(action_dims):\n",
    "            p0_actions[i].append(e.data.action[i].item())\n",
    "    elif e.metadata.winning_player == \"3\":\n",
    "        for i in range(action_dims):\n",
    "            p3_actions[i].append(e.data.action[i].item())\n",
    "\n",
    "# Plot normalized distributions for each dimension separately\n",
    "for i in range(action_dims):\n",
    "    plot_reward_distributions(win_rewards, loss_rewards, \"8794\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa7729-0060-4fa1-b093-393d092547db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Determine the number of dimensions in the state and action tensors\n",
    "state_dims = experiences[0].data.state.shape[-1]\n",
    "action_dims = experiences[0].data.action.shape[-1]\n",
    "\n",
    "# Extract state and action values for each dimension and player group\n",
    "p0_states = [[] for _ in range(state_dims)]\n",
    "p3_states = [[] for _ in range(state_dims)]\n",
    "p0_actions = [[] for _ in range(action_dims)]\n",
    "p3_actions = [[] for _ in range(action_dims)]\n",
    "\n",
    "# Populate state and action lists for each dimension\n",
    "for e in experiences:\n",
    "    if e.metadata.winning_player == \"0\":\n",
    "        for i in range(state_dims):\n",
    "            p0_states[i].append(e.data.state[i].item())\n",
    "        for j in range(action_dims):\n",
    "            p0_actions[j].append(e.data.action[j].item())\n",
    "    elif e.metadata.winning_player == \"3\":\n",
    "        for i in range(state_dims):\n",
    "            p3_states[i].append(e.data.state[i].item())\n",
    "        for j in range(action_dims):\n",
    "            p3_actions[j].append(e.data.action[j].item())\n",
    "\n",
    "# Plot state distributions for each dimension separately (normalized)\n",
    "for i in range(state_dims):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Player 0 state distribution for dimension i\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(p0_states[i], bins=30, alpha=0.7, color='blue', edgecolor='black', density=True)\n",
    "    plt.title(f\"State Distribution for Player 0 (Winning) - Dimension {i+1}\")\n",
    "    plt.xlabel(\"State Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    # Player 3 state distribution for dimension i\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(p3_states[i], bins=30, alpha=0.7, color='green', edgecolor='black', density=True)\n",
    "    plt.title(f\"State Distribution for Player 3 (Losing) - Dimension {i+1}\")\n",
    "    plt.xlabel(\"State Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Overlay state distributions for each dimension (normalized)\n",
    "for i in range(state_dims):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(p0_states[i], bins=60, alpha=0.6, color='blue', edgecolor='black', density=True, label=f\"Winning Game: States - Dimension {i+1}\")\n",
    "    plt.hist(p3_states[i], bins=60, alpha=0.6, color='green', edgecolor='black', density=True, label=f\"Losing Game: States - Dimension {i+1}\")\n",
    "\n",
    "    plt.title(f\"Overlayed State Distributions for Winning and Losing Games - Dimension {i+1}\")\n",
    "    plt.xlabel(\"State Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot action distributions for each dimension separately (normalized)\n",
    "for j in range(action_dims):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Player 0 action distribution for dimension j\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(p0_actions[j], bins=30, alpha=0.7, color='blue', edgecolor='black', density=True)\n",
    "    plt.title(f\"Action Distribution for Player 0 (Winning) - Dimension {j+1}\")\n",
    "    plt.xlabel(\"Action Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    # Player 3 action distribution for dimension j\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(p3_actions[j], bins=30, alpha=0.7, color='green', edgecolor='black', density=True)\n",
    "    plt.title(f\"Action Distribution for Player 3 (Losing) - Dimension {j+1}\")\n",
    "    plt.xlabel(\"Action Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Overlay action distributions for each dimension (normalized)\n",
    "for j in range(action_dims):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(p0_actions[j], bins=60, alpha=0.6, color='blue', edgecolor='black', density=True, label=f\"Winning Game: Actions - Dimension {j+1}\")\n",
    "    plt.hist(p3_actions[j], bins=60, alpha=0.6, color='green', edgecolor='black', density=True, label=f\"Losing Game: Actions - Dimension {j+1}\")\n",
    "\n",
    "    plt.title(f\"Overlayed Action Distributions for Winning and Losing Games - Dimension {j+1}\")\n",
    "    plt.xlabel(\"Action Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897a219-d7e9-4cc7-b50f-5feccb8b1a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60e064-245b-4b19-ade8-f2451da9c65e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
